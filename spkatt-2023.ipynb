{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can set your desired cuda devices for qlora and for inference. Because we clear the VRAM after qlora training, you are able to choose the same devices.\n",
    "We prepared device maps for 7B and 70B models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpus for qlora training\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device map for 7b model\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.layers.0\": 0,\n",
    "    \"model.layers.1\": 0,\n",
    "    \"model.layers.2\": 0,\n",
    "    \"model.layers.3\": 0,\n",
    "    \"model.layers.4\": 0,\n",
    "    \"model.layers.5\": 0,\n",
    "    \"model.layers.6\": 0,\n",
    "    \"model.layers.7\": 0,\n",
    "    \"model.layers.8\": 0,\n",
    "    \"model.layers.9\": 0,\n",
    "    \"model.layers.10\": 0,\n",
    "    \"model.layers.11\": 0,\n",
    "    \"model.layers.12\": 0,\n",
    "    \"model.layers.13\": 0,\n",
    "    \"model.layers.14\": 0,\n",
    "    \"model.layers.15\": 0,\n",
    "    \"model.layers.16\": 0,\n",
    "    \"model.layers.17\": 0,\n",
    "    \"model.layers.18\": 0,\n",
    "    \"model.layers.19\": 0,\n",
    "    \"model.layers.20\": 0,\n",
    "    \"model.layers.21\": 0,\n",
    "    \"model.layers.22\": 0,\n",
    "    \"model.layers.23\": 0,\n",
    "    \"model.layers.24\": 0,\n",
    "    \"model.layers.25\": 0,\n",
    "    \"model.layers.26\": 0,\n",
    "    \"model.layers.27\": 0,\n",
    "    \"model.layers.28\": 0,\n",
    "    \"model.layers.29\": 0,\n",
    "    \"model.layers.30\": 0,\n",
    "    \"model.layers.31\": 0,\n",
    "    \"model.norm\": 0,\n",
    "    \"lm_head\": 0,\n",
    "}\n",
    "\n",
    "# device map for 70b model\n",
    "# device_map = {\n",
    "#     \"model.embed_tokens\": 0,\n",
    "#     \"model.layers.0\": 0,\n",
    "#     \"model.layers.1\": 0,\n",
    "#     \"model.layers.2\": 0,\n",
    "#     \"model.layers.3\": 0,\n",
    "#     \"model.layers.4\": 0,\n",
    "#     \"model.layers.5\": 0,\n",
    "#     \"model.layers.6\": 0,\n",
    "#     \"model.layers.7\": 0,\n",
    "#     \"model.layers.8\": 0,\n",
    "#     \"model.layers.9\": 0,\n",
    "#     \"model.layers.10\": 0,\n",
    "#     \"model.layers.11\": 0,\n",
    "#     \"model.layers.12\": 0,\n",
    "#     \"model.layers.13\": 0,\n",
    "#     \"model.layers.14\": 0,\n",
    "#     \"model.layers.15\": 0,\n",
    "#     \"model.layers.16\": 0,\n",
    "#     \"model.layers.17\": 0,\n",
    "#     \"model.layers.18\": 1,\n",
    "#     \"model.layers.19\": 1,\n",
    "#     \"model.layers.20\": 1,\n",
    "#     \"model.layers.21\": 1,\n",
    "#     \"model.layers.22\": 1,\n",
    "#     \"model.layers.23\": 1,\n",
    "#     \"model.layers.24\": 1,\n",
    "#     \"model.layers.25\": 1,\n",
    "#     \"model.layers.26\": 1,\n",
    "#     \"model.layers.27\": 1,\n",
    "#     \"model.layers.28\": 1,\n",
    "#     \"model.layers.29\": 1,\n",
    "#     \"model.layers.30\": 1,\n",
    "#     \"model.layers.31\": 1,\n",
    "#     \"model.layers.32\": 1,\n",
    "#     \"model.layers.33\": 1,\n",
    "#     \"model.layers.34\": 1,\n",
    "#     \"model.layers.35\": 1,\n",
    "#     \"model.layers.36\": 1,\n",
    "#     \"model.layers.37\": 1,\n",
    "#     \"model.layers.38\": 1,\n",
    "#     \"model.layers.39\": 2,\n",
    "#     \"model.layers.40\": 2,\n",
    "#     \"model.layers.41\": 2,\n",
    "#     \"model.layers.42\": 2,\n",
    "#     \"model.layers.43\": 2,\n",
    "#     \"model.layers.44\": 2,\n",
    "#     \"model.layers.45\": 2,\n",
    "#     \"model.layers.46\": 2,\n",
    "#     \"model.layers.47\": 2,\n",
    "#     \"model.layers.48\": 2,\n",
    "#     \"model.layers.49\": 2,\n",
    "#     \"model.layers.50\": 2,\n",
    "#     \"model.layers.51\": 2,\n",
    "#     \"model.layers.52\": 2,\n",
    "#     \"model.layers.53\": 2,\n",
    "#     \"model.layers.54\": 2,\n",
    "#     \"model.layers.55\": 2,\n",
    "#     \"model.layers.56\": 2,\n",
    "#     \"model.layers.57\": 2,\n",
    "#     \"model.layers.58\": 2,\n",
    "#     \"model.layers.59\": 2,\n",
    "#     \"model.layers.60\": 3,\n",
    "#     \"model.layers.61\": 3,\n",
    "#     \"model.layers.62\": 3,\n",
    "#     \"model.layers.63\": 3,\n",
    "#     \"model.layers.64\": 3,\n",
    "#     \"model.layers.65\": 3,\n",
    "#     \"model.layers.66\": 3,\n",
    "#     \"model.layers.67\": 3,\n",
    "#     \"model.layers.68\": 3,\n",
    "#     \"model.layers.69\": 3,\n",
    "#     \"model.layers.70\": 3,\n",
    "#     \"model.layers.71\": 3,\n",
    "#     \"model.layers.72\": 3,\n",
    "#     \"model.layers.73\": 3,\n",
    "#     \"model.layers.74\": 3,\n",
    "#     \"model.layers.75\": 3,\n",
    "#     \"model.layers.76\": 3,\n",
    "#     \"model.layers.77\": 3,\n",
    "#     \"model.layers.78\": 3,\n",
    "#     \"model.layers.79\": 3,\n",
    "#     \"model.norm\": 3,\n",
    "#     \"lm_head\": 3,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "import Levenshtein\n",
    "import shutil\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    concatenate_datasets,\n",
    "    load_from_disk,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    ")\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, LlamaTokenizer\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from peft import PeftModel\n",
    "\n",
    "from datasets import logging as ds_logging\n",
    "from transformers import logging as trans_logging\n",
    "\n",
    "from qlora import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress logging from common libs to keep outputs clean\n",
    "ds_logging.set_verbosity_error()\n",
    "ds_logging.disable_progress_bar()\n",
    "trans_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_file(path: str, file: str):\n",
    "    \"\"\"Read \"Annotations\" field of given file and return all annotations as a hf dataset.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to file\n",
    "        file (str): filename\n",
    "\n",
    "    Returns:\n",
    "        ds (datasets.Dataset): All annotations of given file\n",
    "    \"\"\"\n",
    "    features = Features(\n",
    "        {\n",
    "            \"PTC\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Evidence\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Medium\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Topic\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Cue\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Addr\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Message\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Source\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(path, file),\n",
    "        field=\"Annotations\",\n",
    "        split=\"train\",\n",
    "        features=features,\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(path: str, file: str):\n",
    "    \"\"\"Read \"Sentences\" field of given file and return all sentences as a hf dataset.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to file\n",
    "        file (str): filename\n",
    "\n",
    "    Returns:\n",
    "        ds (datasets.Dataset): All sentences of given file\n",
    "    \"\"\"\n",
    "    ds = load_dataset(\n",
    "        \"json\", data_files=os.path.join(path, file), field=\"Sentences\", split=\"train\"\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    ds = ds.add_column(\"Sentence\", [\" \".join(t) for t in ds[\"Tokens\"]])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_path(path: str):\n",
    "    \"\"\"Read all annotations from all files in given path.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to files\n",
    "\n",
    "    Returns:\n",
    "        dataset (datasets.Dataset): All annotations of all files in given path\n",
    "    \"\"\"\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_annotations_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_annotations_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_path(path: str):\n",
    "    \"\"\"Read all sentences from all files in given path.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to files\n",
    "\n",
    "    Returns:\n",
    "        dataset (datasets.Dataset): All sentences of all files in given path\n",
    "    \"\"\"\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_sentences_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_sentences_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    # Add id to later identify the origin of a given token\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_dataset(ds_name: str):\n",
    "    \"\"\"Read all annotations from the given dataset split and cache them. Returns cache if it exists.\n",
    "\n",
    "    Args:\n",
    "        ds_name (str): name of dataset split to read and name of the cache entry\n",
    "\n",
    "    Returns:\n",
    "        result (datasets.Dataset): All annotations of the dataset split\n",
    "    \"\"\"\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    result = read_annotations_from_path(\n",
    "        \"./SpkAtt-2023/data/\"\n",
    "        + ds_name\n",
    "        + \"/task1\"\n",
    "        + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "    )\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_dataset(ds_name: str):\n",
    "    \"\"\"Read all sentences from the given dataset split and cache them. Returns cache if it exists.\n",
    "\n",
    "    Args:\n",
    "        ds_name (str): name of dataset split to read and name of the cache entry\n",
    "\n",
    "    Returns:\n",
    "        result (datasets.Dataset): All sentences of the dataset split\n",
    "    \"\"\"\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        result = load_from_disk(path_to_dataset)\n",
    "    else:\n",
    "        result = read_sentences_from_path(\n",
    "            \"./SpkAtt-2023/data/\"\n",
    "            + ds_name\n",
    "            + \"/task1\"\n",
    "            + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "        )\n",
    "        os.makedirs(path_to_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all sentences of all dataset splits\n",
    "train_sentences_dataset = read_sentences_dataset(\"train\")\n",
    "val_sentences_dataset = read_sentences_dataset(\"dev\")\n",
    "test_sentences_dataset = read_sentences_dataset(\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all annotations of all dataset splits\n",
    "train_annotations_dataset = read_annotations_dataset(\"train\")\n",
    "val_annotations_dataset = read_annotations_dataset(\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format datasets for usage in langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_label(sentences_dataset, row, annotation):\n",
    "    \"\"\"Convert annotations to the corresponding tokens\n",
    "\n",
    "    Args:\n",
    "        sentences_dataset (datasets.Dataset): sentences dataset containing the token lists for each sample\n",
    "        row (dict): current row\n",
    "        annotation (list): List of all token references in current annotation\n",
    "\n",
    "    Returns:\n",
    "        tokens (list): list of corresponding tokens from annotation\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for anno in annotation:\n",
    "        if int(anno.split(\":\")[0]) == row[\"SentenceId\"]:\n",
    "            # anno references token from current sample (row)\n",
    "            tokens.append(row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "        else:\n",
    "            # anno references token from another sample\n",
    "            temp_row = sentences_dataset.filter(\n",
    "                lambda r: r[\"FileName\"] == row[\"FileName\"]\n",
    "                and r[\"SentenceId\"] == int(anno.split(\":\")[0])\n",
    "            )[0]\n",
    "            tokens.append(temp_row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_dataset(sentences_dataset, annotations_dataset, dataset_name):\n",
    "    \"\"\"Combines sentences dataset and annotations dataset to one dataset for given dataset split\n",
    "\n",
    "    Args:\n",
    "        sentences_dataset (datasets.Dataset): sentences dataset of split\n",
    "        annotations_dataset (datasets.Dataset): annotations dataset of split\n",
    "        dataset_name (str): name of dataset split\n",
    "\n",
    "    Returns:\n",
    "        res (datasets.Dataset): combined dataset (see \"Dataset Showcase\" for examples)\n",
    "    \"\"\"\n",
    "    path_to_dataset = \"./transformed_datasets/\" + dataset_name + \"/complete\"\n",
    "\n",
    "    # return cache if exists\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    # temp lists\n",
    "    ptc, ptc_temp, ptc_mapped, ptc_mapped_temp = [], [], [], []\n",
    "    evidence, evidence_temp, evidence_mapped, evidence_mapped_temp = [], [], [], []\n",
    "    medium, medium_temp, medium_mapped, medium_mapped_temp = [], [], [], []\n",
    "    topic, topic_temp, topic_mapped, topic_mapped_temp = [], [], [], []\n",
    "    cue, cue_temp, cue_mapped, cue_mapped_temp = [], [], [], []\n",
    "    addr, addr_temp, addr_mapped, addr_mapped_temp = [], [], [], []\n",
    "    message, message_temp, message_mapped, message_mapped_temp = [], [], [], []\n",
    "    source, source_temp, source_mapped, source_mapped_temp = [], [], [], []\n",
    "    sentence_extended, tokens_extended, sentence_extended_ids = [], [], []\n",
    "\n",
    "    index_in_anno_ds = 0\n",
    "\n",
    "    for i, row in tqdm(enumerate(sentences_dataset)):\n",
    "        context = row[\"Sentence\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        ids = [row[\"SentenceId\"]] * len(row[\"Tokens\"])\n",
    "\n",
    "        # extend context of sentence with the sentence of the following two samples if possible\n",
    "        if (\n",
    "            i + 1 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 1][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 1][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 1][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            )\n",
    "        if (\n",
    "            i + 2 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 2][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 2][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 2][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            )\n",
    "        sentence_extended.append(context)\n",
    "        tokens_extended.append(tokens)\n",
    "        sentence_extended_ids.append(ids)\n",
    "\n",
    "        # iterate annotations dataset and assign annotations to their sentences\n",
    "        if annotations_dataset is not None:\n",
    "            id_of_next_sentence_with_annotation = (\n",
    "                int(annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0])\n",
    "                if index_in_anno_ds != len(annotations_dataset)\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            # reset temp lists\n",
    "            if row[\"SentenceId\"] != id_of_next_sentence_with_annotation:\n",
    "                ptc.append([])\n",
    "                ptc_mapped.append([])\n",
    "                evidence.append([])\n",
    "                evidence_mapped.append([])\n",
    "                medium.append([])\n",
    "                medium_mapped.append([])\n",
    "                topic.append([])\n",
    "                topic_mapped.append([])\n",
    "                cue.append([])\n",
    "                cue_mapped.append([])\n",
    "                addr.append([])\n",
    "                addr_mapped.append([])\n",
    "                message.append([])\n",
    "                message_mapped.append([])\n",
    "                source.append([])\n",
    "                source_mapped.append([])\n",
    "                continue\n",
    "\n",
    "            while row[\"SentenceId\"] == id_of_next_sentence_with_annotation:\n",
    "                # save raw annotations\n",
    "                ptc_temp.append(annotations_dataset[index_in_anno_ds][\"PTC\"])\n",
    "                evidence_temp.append(annotations_dataset[index_in_anno_ds][\"Evidence\"])\n",
    "                medium_temp.append(annotations_dataset[index_in_anno_ds][\"Medium\"])\n",
    "                topic_temp.append(annotations_dataset[index_in_anno_ds][\"Topic\"])\n",
    "                cue_temp.append(annotations_dataset[index_in_anno_ds][\"Cue\"])\n",
    "                addr_temp.append(annotations_dataset[index_in_anno_ds][\"Addr\"])\n",
    "                message_temp.append(annotations_dataset[index_in_anno_ds][\"Message\"])\n",
    "                source_temp.append(annotations_dataset[index_in_anno_ds][\"Source\"])\n",
    "\n",
    "                # save annotations after transformation to token lists\n",
    "                ptc_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, ptc_temp[-1])\n",
    "                )\n",
    "                evidence_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, evidence_temp[-1])\n",
    "                )\n",
    "                medium_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, medium_temp[-1])\n",
    "                )\n",
    "                topic_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, topic_temp[-1])\n",
    "                )\n",
    "                cue_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, cue_temp[-1])\n",
    "                )\n",
    "                addr_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, addr_temp[-1])\n",
    "                )\n",
    "                message_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, message_temp[-1])\n",
    "                )\n",
    "                source_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, source_temp[-1])\n",
    "                )\n",
    "\n",
    "                index_in_anno_ds += 1\n",
    "                if index_in_anno_ds == len(annotations_dataset):\n",
    "                    break\n",
    "                id_of_next_sentence_with_annotation = int(\n",
    "                    annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0]\n",
    "                )\n",
    "\n",
    "            ptc.append(ptc_temp)\n",
    "            ptc_mapped.append(ptc_mapped_temp)\n",
    "            evidence.append(evidence_temp)\n",
    "            evidence_mapped.append(evidence_mapped_temp)\n",
    "            medium.append(medium_temp)\n",
    "            medium_mapped.append(medium_mapped_temp)\n",
    "            topic.append(topic_temp)\n",
    "            topic_mapped.append(topic_mapped_temp)\n",
    "            cue.append(cue_temp)\n",
    "            cue_mapped.append(cue_mapped_temp)\n",
    "            addr.append(addr_temp)\n",
    "            addr_mapped.append(addr_mapped_temp)\n",
    "            message.append(message_temp)\n",
    "            message_mapped.append(message_mapped_temp)\n",
    "            source.append(source_temp)\n",
    "            source_mapped.append(source_mapped_temp)\n",
    "\n",
    "            ptc_temp, ptc_mapped_temp = [], []\n",
    "            evidence_temp, evidence_mapped_temp = [], []\n",
    "            medium_temp, medium_mapped_temp = [], []\n",
    "            topic_temp, topic_mapped_temp = [], []\n",
    "            cue_temp, cue_mapped_temp = [], []\n",
    "            addr_temp, addr_mapped_temp = [], []\n",
    "            message_temp, message_mapped_temp = [], []\n",
    "            source_temp, source_mapped_temp = [], []\n",
    "\n",
    "    # build result dataset\n",
    "    res = sentences_dataset.add_column(\"sentence_extended\", sentence_extended)\n",
    "    res = res.add_column(\"tokens_extended\", tokens_extended)\n",
    "    res = res.add_column(\"sentence_extended_ids\", sentence_extended_ids)\n",
    "\n",
    "    if annotations_dataset is not None:\n",
    "        res = res.add_column(\"ptc\", ptc)\n",
    "        res = res.add_column(\"ptc_mapped\", ptc_mapped)\n",
    "        res = res.add_column(\"evidence\", evidence)\n",
    "        res = res.add_column(\"evidence_mapped\", evidence_mapped)\n",
    "        res = res.add_column(\"medium\", medium)\n",
    "        res = res.add_column(\"medium_mapped\", medium_mapped)\n",
    "        res = res.add_column(\"topic\", topic)\n",
    "        res = res.add_column(\"topic_mapped\", topic_mapped)\n",
    "        res = res.add_column(\"cue\", cue)\n",
    "        res = res.add_column(\"cue_mapped\", cue_mapped)\n",
    "        res = res.add_column(\"addr\", addr)\n",
    "        res = res.add_column(\"addr_mapped\", addr_mapped)\n",
    "        res = res.add_column(\"message\", message)\n",
    "        res = res.add_column(\"message_mapped\", message_mapped)\n",
    "        res = res.add_column(\"source\", source)\n",
    "        res = res.add_column(\"source_mapped\", source_mapped)\n",
    "\n",
    "    # save dataset cache\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    res.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all complete datasets of all dataset splits\n",
    "train_ds = build_complete_dataset(\n",
    "    train_sentences_dataset, train_annotations_dataset, \"train\"\n",
    ")\n",
    "val_ds = build_complete_dataset(val_sentences_dataset, val_annotations_dataset, \"dev\")\n",
    "test_ds = build_complete_dataset(test_sentences_dataset, None, \"eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning\n",
    "\n",
    "## Parse data into required format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cues_to_string(mapped):\n",
    "    \"\"\"Transform all cues of a sample to the desired format\n",
    "\n",
    "    Args:\n",
    "        mapped (list): list of all cues\n",
    "\n",
    "    Returns:\n",
    "        _ (str): string containing all cues of a sample in the desired format\n",
    "    \"\"\"\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join([\"[\" + \", \".join(val) + \"]\" for val in mapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_roles_to_string(mapped):\n",
    "    \"\"\"Transform a role of a sample to the desired format\n",
    "\n",
    "    Args:\n",
    "        mapped (list): list of tokens for an annotation\n",
    "\n",
    "    Returns:\n",
    "        _ (str): string containing all tokens of given annotation in the desired format\n",
    "    \"\"\"\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_cues_file = \"./transformed_datasets/prompts_training/parsed_data_cues.jsonl\"\n",
    "parsed_roles_file = \"./transformed_datasets/prompts_training/parsed_data_roles.jsonl\"\n",
    "os.makedirs(os.path.dirname(parsed_cues_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(parsed_roles_file), exist_ok=True)\n",
    "\n",
    "# token to signal the end of the assistant's response\n",
    "separator = \"</s>\"\n",
    "\n",
    "# save parsed prompts separately\n",
    "all_prompts_cues = []\n",
    "all_prompts_roles = []\n",
    "\n",
    "for row in concatenate_datasets([train_ds, val_ds]):\n",
    "    # keep track of the complete conversation in order to generate the input of the prompts\n",
    "    # user prompt for cues\n",
    "    complete_prompt = \"User: \"\n",
    "    complete_prompt += 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "    complete_prompt += row[\"Sentence\"]\n",
    "    # assistant response with cues --> create sample with the conversation up to this point as input and the cues as output\n",
    "    complete_prompt += \"\\nAssistant: \"\n",
    "    cue_sample = json.dumps(\n",
    "        {\n",
    "            \"input\": complete_prompt,\n",
    "            \"output\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]) + separator,\n",
    "        }\n",
    "    )\n",
    "    if cue_sample not in all_prompts_cues:\n",
    "        all_prompts_cues.append(cue_sample)\n",
    "    complete_prompt += \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]) + separator\n",
    "\n",
    "    for i, cue in enumerate(row[\"cue_mapped\"]):\n",
    "        # user prompt for roles for one specific cue\n",
    "        complete_prompt_role = complete_prompt + \"\\nUser: \"\n",
    "        complete_prompt_role += \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: \"\n",
    "        complete_prompt_role += row[\"sentence_extended\"]\n",
    "        complete_prompt_role += (\n",
    "            \"\\n\\nNow find all roles in the sentence associated with the cue '\"\n",
    "        )\n",
    "        complete_prompt_role += \", \".join(cue)\n",
    "        complete_prompt_role += \"' you found in the beginning sentence.\"\n",
    "\n",
    "        # assistant response with roles --> create sample with the conversation up to this point as input and the roles as output\n",
    "        complete_prompt_role += \"\\nAssistant: \"\n",
    "        roles_sample = json.dumps(\n",
    "            {\n",
    "                \"input\": complete_prompt_role,\n",
    "                \"output\": \"cue: \"\n",
    "                + \", \".join(cue)\n",
    "                + \"\\nptc: \"\n",
    "                + map_roles_to_string(row[\"ptc_mapped\"][i])\n",
    "                + \"\\nevidence: \"\n",
    "                + map_roles_to_string(row[\"evidence_mapped\"][i])\n",
    "                + \"\\nmedium: \"\n",
    "                + map_roles_to_string(row[\"medium_mapped\"][i])\n",
    "                + \"\\ntopic: \"\n",
    "                + map_roles_to_string(row[\"topic_mapped\"][i])\n",
    "                + \"\\naddr: \"\n",
    "                + map_roles_to_string(row[\"addr_mapped\"][i])\n",
    "                + \"\\nmessage: \"\n",
    "                + map_roles_to_string(row[\"message_mapped\"][i])\n",
    "                + \"\\nsource: \"\n",
    "                + map_roles_to_string(row[\"source_mapped\"][i])\n",
    "                + separator,\n",
    "            }\n",
    "        )\n",
    "        if roles_sample not in all_prompts_roles:\n",
    "            all_prompts_roles.append(roles_sample)\n",
    "\n",
    "# write parsed prompts to files\n",
    "with open(parsed_cues_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(all_prompts_cues))\n",
    "with open(parsed_roles_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(all_prompts_roles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the file with the cue prompts was written correctly\n",
    "with open(parsed_cues_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Number of samples: {len(lines)}\\n\")\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for l in lines[:5]:\n",
    "    print(\"=== in: ===\\n\" + json.loads(l)[\"input\"] + \"\\n\")\n",
    "    print(\"=== out: ===\\n\" + json.loads(l)[\"output\"] + \"\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the file with the role prompts was written correctly\n",
    "with open(parsed_roles_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Number of samples: {len(lines)}\\n\")\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for l in lines[:5]:\n",
    "    print(\"=== in: ===\\n\" + json.loads(l)[\"input\"] + \"\\n\")\n",
    "    print(\"=== out: ===\\n\" + json.loads(l)[\"output\"] + \"\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check optimal source and target lengths\n",
    "\n",
    "This step is only required if you want to use your own data. If you use the original GermEval 2023 task 1 data, you can skip this step and use the source and target lengths that are already defined in the configurations below at the start of the training code (parameters `source_max_len` and `target_max_len`).\n",
    "\n",
    "If you want to change the maximum source or target lengths, keep in mind that longer prompts mean longer training times and more memory requirements. While it would be best to set the maximum source/target lengths to the maximum lengths of the inputs/outputs, this is not always feasible due to memory constraints. In this case, we recommend choosing maximum lengths that only truncate few samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode all prompt inputs with the Llama 1 tokenizer (same as the Llama 2 tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\", padding_side=\"right\", use_fast=False, tokenizer_type=\"llama\"\n",
    ")\n",
    "\n",
    "encoded_inputs_cues = []\n",
    "encoded_inputs_roles = []\n",
    "encoded_outputs_cues = []\n",
    "encoded_outputs_roles = []\n",
    "with open(parsed_cues_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)[\"input\"])\n",
    "        encoded_inputs_cues.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)[\"output\"])\n",
    "        encoded_outputs_cues.append(enc_out)\n",
    "with open(parsed_roles_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)[\"input\"])\n",
    "        encoded_inputs_roles.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)[\"output\"])\n",
    "        encoded_outputs_roles.append(enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maximum source lengths taken from the config files\n",
    "max_length_source_cues = 256\n",
    "max_length_source_roles = 640\n",
    "\n",
    "print(\"cues source lengths\")\n",
    "len_enc = [len(e) for e in encoded_inputs_cues]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_source_cues}: {sum(np.array(len_enc) > max_length_source_cues)}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"roles source lengths\")\n",
    "len_enc = [len(e) for e in encoded_inputs_roles]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_source_roles}: {sum(np.array(len_enc) > max_length_source_roles)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maximum target lengths taken from the config files\n",
    "max_length_target_cues = 64\n",
    "max_length_target_roles = 256\n",
    "\n",
    "print(\"cues target lengths\")\n",
    "len_enc = [len(e) for e in encoded_outputs_cues]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_target_cues}: {sum(np.array(len_enc) > max_length_target_cues)}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"roles target lengths\")\n",
    "len_enc = [len(e) for e in encoded_outputs_roles]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_target_roles}: {sum(np.array(len_enc) > max_length_target_roles)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train models\n",
    "\n",
    "This step can be skipped if you already have trained models.\n",
    "\n",
    "For training, you first have to prepare the Llama 2 models and adapt the configuration. To prepare the Llama 2 models, you will have to make them accessible in HF (Huggingface) format. You can either use the models directly from Huggingface or prepare them yourself by first downloading the model weights from [the official Llama repo](https://github.com/facebookresearch/llama) and then converting these weights using their [conversion manual](https://github.com/facebookresearch/llama-recipes/#model-conversion-to-hugging-face). When using the models from Huggingface, you should add the parameter `use_auth_token` with your Huggingface token to the training configs in the code cell below. If you don't want to use the models from Huggingface, once you have prepared the models yourself, update the path to the models in the config (parameter `model_name_or_path`) so the paths point to the folder containing the `pytorch_model-000xx-of-00015.bin` files.\n",
    "\n",
    "Further configuration parameters:\n",
    "\n",
    "- `per_device_train_batch_size` and `gradient_accumulation_steps`: With these two parameters you can control the batch size and the number of accumulation steps when calculating the gradients during training. Larger batch sizes should speed up training, but increase memory requirements considerably. We recommend choosing the parameters so that their product `per_device_train_batch_size * gradient_accumulation_steps` is a multiple of 16.\n",
    "- `save_steps` and `max_steps`: set `max_steps` to control the length of training (`save_steps` determines when checkpoints are created)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config files for training\n",
    "# 7B models\n",
    "cues_training_config = {\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"output_dir\": \"./output/spkatt-7b-cues\",\n",
    "    \"data_seed\": 42,\n",
    "    \"save_steps\": 500,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lora_modules\": \"all\",\n",
    "    \"bf16\": True,\n",
    "    \"dataset\": \"transformed_datasets/prompts_training/parsed_data_cues.jsonl\",\n",
    "    \"dataset_format\": \"input-output\",\n",
    "    \"source_max_len\": 256,\n",
    "    \"target_max_len\": 64,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_steps\": 2000,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "roles_training_config = {\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"output_dir\": \"./output/spkatt-7b-roles\",\n",
    "    \"data_seed\": 42,\n",
    "    \"save_steps\": 500,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lora_modules\": \"all\",\n",
    "    \"bf16\": True,\n",
    "    \"dataset\": \"transformed_datasets/prompts_training/parsed_data_roles.jsonl\",\n",
    "    \"dataset_format\": \"input-output\",\n",
    "    \"source_max_len\": 640,\n",
    "    \"target_max_len\": 256,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_steps\": 2000,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "# 70B models\n",
    "# cues_training_config = {\"model_name_or_path\": \"meta-llama/Llama-2-70b-hf\",\n",
    "#                         \"output_dir\": \"./output/spkatt-70b-cues\",\n",
    "#                         \"data_seed\": 42,\n",
    "#                         \"save_steps\": 500,\n",
    "#                         \"evaluation_strategy\": \"no\",\n",
    "#                         \"dataloader_num_workers\": 4,\n",
    "#                         \"lora_modules\": \"all\",\n",
    "#                         \"bf16\": True,\n",
    "#                         \"dataset\": \"transformed_datasets/prompts_training/parsed_data_cues.jsonl\",\n",
    "#                         \"dataset_format\": \"input-output\",\n",
    "#                         \"source_max_len\": 256,\n",
    "#                         \"target_max_len\": 64,\n",
    "#                         \"per_device_train_batch_size\": 16,\n",
    "#                         \"gradient_accumulation_steps\": 1,\n",
    "#                         \"max_steps\": 2000,\n",
    "#                         \"learning_rate\": 0.0001,\n",
    "#                         \"lora_dropout\": 0.05,\n",
    "#                         \"seed\": 0,\n",
    "#                         }\n",
    "# roles_training_config = {\"model_name_or_path\": \"meta-llama/Llama-2-70b-hf\",\n",
    "#                          \"output_dir\": \"./output/spkatt-70b-roles\",\n",
    "#                          \"data_seed\": 42,\n",
    "#                          \"save_steps\": 500,\n",
    "#                          \"evaluation_strategy\": \"no\",\n",
    "#                          \"dataloader_num_workers\": 4,\n",
    "#                          \"lora_modules\": \"all\",\n",
    "#                          \"bf16\": True,\n",
    "#                          \"dataset\": \"transformed_datasets/prompts_training/parsed_data_roles.jsonl\",\n",
    "#                          \"dataset_format\": \"input-output\",\n",
    "#                          \"source_max_len\": 640,\n",
    "#                          \"target_max_len\": 256,\n",
    "#                          \"per_device_train_batch_size\": 8,\n",
    "#                          \"gradient_accumulation_steps\": 2,\n",
    "#                          \"max_steps\": 2500,\n",
    "#                          \"learning_rate\": 0.0001,\n",
    "#                          \"lora_dropout\": 0.05,\n",
    "#                          \"seed\": 0,\n",
    "#                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(cues_training_config)\n",
    "\n",
    "# free vram after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(roles_training_config)\n",
    "\n",
    "# free vram after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for Cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cue model for inference\n",
    "\n",
    "You can choose the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model from config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cues_training_config[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Load fine tuned model\n",
    "checkpoint_dir = (\n",
    "    cues_training_config[\"output_dir\"] + \"/checkpoint-2000/\"\n",
    ")  # choose checkpoint\n",
    "model = PeftModel.from_pretrained(model, os.path.join(checkpoint_dir, \"adapter_model\"))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    cues_training_config[\"model_name_or_path\"], legacy=False\n",
    ")\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "# set langchain pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Cue-LLM-Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for cues\n",
    "template_cues = \"\"\"User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
    "I want you to extract all cues in the text below.\n",
    "If you find multiple words for one cue, you output them separated by commas.\n",
    "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
    "Now extract all cues from the following sentence.\n",
    "Use the prefix \\\"Cues: \\\".\n",
    "Sentence: {Sentence}\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set langchain llm chain\n",
    "prompt_cues = PromptTemplate(input_variables=[\"Sentence\"], template=template_cues)\n",
    "llm_chain_cues = LLMChain(prompt=prompt_cues, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_cues = []\n",
    "for row in tqdm(test_sentences_dataset, desc=\"Cues\"):\n",
    "    outputs_cues.append(llm_chain_cues.apply([row])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free vram after inference for cues\n",
    "del tokenizer\n",
    "del model\n",
    "del pipe\n",
    "del llm\n",
    "del llm_chain_cues\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and extract cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data in correct format and insert raw cue outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_to_output_file_format(inputs, outputs):\n",
    "    \"\"\"Iterates over files in the given dataset and the generated outputs\n",
    "       to build the submission format as python dict.\n",
    "       The raw outputs are appended for later usage.\n",
    "\n",
    "    Args:\n",
    "        inputs (datasets.Dataset): dataset that was used for inference\n",
    "        outputs (_type_): outputs for given dataset\n",
    "\n",
    "    Returns:\n",
    "        result (datasets.Dataset): dataset split by files contained in one dict with appended raw outputs\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    seen_sentences = []\n",
    "\n",
    "    for i, row in enumerate(inputs):\n",
    "        # Add file to result dict if not already included\n",
    "        if row[\"FileName\"] not in result:\n",
    "            result[row[\"FileName\"]] = {\n",
    "                \"Sentences\": [],\n",
    "                \"Annotations\": [],\n",
    "                \"Outputs\": {\"Cues\": {}},\n",
    "            }\n",
    "\n",
    "        # sentence of current row is first seen in this iteration => add this sentence to \"Sentences\" field\n",
    "        if row[\"FileName\"] + \"-\" + str(row[\"SentenceId\"]) not in seen_sentences:\n",
    "            seen_sentences.append(row[\"FileName\"] + \"-\" + str(row[\"SentenceId\"]))\n",
    "            result[row[\"FileName\"]][\"Sentences\"].append(\n",
    "                {\"SentenceId\": row[\"SentenceId\"], \"Tokens\": row[\"Tokens\"]}\n",
    "            )\n",
    "\n",
    "        # Add new empty list for sentence in field for raw outputs if it does not already exist\n",
    "        if row[\"SentenceId\"] not in result[row[\"FileName\"]][\"Outputs\"][\"Cues\"]:\n",
    "            result[row[\"FileName\"]][\"Outputs\"][\"Cues\"][row[\"SentenceId\"]] = []\n",
    "\n",
    "        # Append raw output for the current row to the list for raw outputs\n",
    "        result[row[\"FileName\"]][\"Outputs\"][\"Cues\"][row[\"SentenceId\"]].append(\n",
    "            outputs[i][\"text\"]\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs_to_output_files(inputs, outputs):\n",
    "    \"\"\"Iterates over files in the given dataset and the generated outputs\n",
    "       to build the submission format and appends the raw outputs for later usage.\n",
    "       Each file is saved separately.\n",
    "\n",
    "    Args:\n",
    "        inputs (datasets.Dataset): dataset that was used for inference\n",
    "        outputs (_type_): outputs for given dataset\n",
    "    \"\"\"\n",
    "    path = \"./output/data/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for key, value in map_outputs_to_output_file_format(inputs, outputs).items():\n",
    "        with open(path + key, \"w\", encoding=\"utf8\") as outfile:\n",
    "            # dump json in output dir\n",
    "            json.dump(value, outfile, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset in correct submission format and append raw outputs for cues\n",
    "save_outputs_to_output_files(test_sentences_dataset, outputs_cues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map raw cue outputs to cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_overlap(cues):\n",
    "    \"\"\"Checks if there are two cues in the given list of cues that have the same tokens in them.\n",
    "\n",
    "    Args:\n",
    "        cues (list): list of cues\n",
    "\n",
    "    Returns:\n",
    "        boolean: indicator of an overlap was found\n",
    "        integer: index of first cue for which an overlap was found (-1 if no overlap was found)\n",
    "        integer: index of second cue for which an overlap was found (-1 if no overlap was found)\n",
    "    \"\"\"\n",
    "    for i, cue in enumerate(cues):\n",
    "        for j in range(i + 1, len(cues)):\n",
    "            if len(list(set(cue) & set(cues[j]))) > 0:\n",
    "                return True, i, j\n",
    "    return False, -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cues_from_output(output_string: str):\n",
    "    \"\"\"Extracts detected cues from raw outputs from inference.\n",
    "\n",
    "    Args:\n",
    "        output_string (str): raw output of model\n",
    "\n",
    "    Raises:\n",
    "        SystemError: Raised when output does not start with \"Cues:\"\n",
    "        LookupError: Raised when output does not contain expected brackets\n",
    "\n",
    "    Returns:\n",
    "        cues (list): list of extracted cues\n",
    "    \"\"\"\n",
    "\n",
    "    # Raise error when output does not start with \"Cues:\"\n",
    "    output_string = output_string.strip().split(\"\\n\")[0].strip()\n",
    "    if output_string.startswith(\"Cues:\"):\n",
    "        output_string = output_string[5:].strip()\n",
    "    else:\n",
    "        raise SystemError\n",
    "\n",
    "    # if the output is empty or the #UNK# marker there were no found cues\n",
    "    if output_string == \"\" or output_string == \"#UNK#\":\n",
    "        return []\n",
    "\n",
    "    outputs = [v.strip() for v in output_string.strip().split(\"],\")]\n",
    "\n",
    "    cues = []\n",
    "    # iterate over every detected cue\n",
    "    for i, output in enumerate(outputs):\n",
    "        # fix missing brackets from splitting\n",
    "        if i < len(outputs) - 1:\n",
    "            output = output + \"]\"\n",
    "        # raise error when output does not contain expected brackets\n",
    "        if not output.startswith(\"[\") or not output.endswith(\"]\"):\n",
    "            raise LookupError\n",
    "        # remove brackets and split words of cue\n",
    "        output = output[1:-1]\n",
    "        output = [v.strip().split(\" \")[0].strip() for v in output.strip().split(\",\")]\n",
    "\n",
    "        # remove #UNK# marker if its contained in cue\n",
    "        while \"#UNK#\" in output:\n",
    "            output.pop(output.index(\"#UNK#\"))\n",
    "\n",
    "        cues.append(output)\n",
    "\n",
    "    # check for overlaps in cue list and combine cues with overlaps\n",
    "    overlap, i, j = check_for_overlap(cues)\n",
    "    while overlap:\n",
    "        cue_2 = cues.pop(j)\n",
    "        cue_1 = cues.pop(i)\n",
    "        cue_1.extend(cue_2)\n",
    "        cue_1 = list(set(cue_1))\n",
    "        cues.append(cue_1)\n",
    "\n",
    "        overlap, i, j = check_for_overlap(cues)\n",
    "\n",
    "    return cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cues():\n",
    "    \"\"\"Iterate over all files in output dir and extract cues from raw outputs\n",
    "\n",
    "    Returns:\n",
    "        count_cues (integer): count of detected cues\n",
    "    \"\"\"\n",
    "    path = \"./output/data/\"\n",
    "    count_cues = 0\n",
    "\n",
    "    # iterate over files\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        # ignore zip files\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            # load file and save extracted cues in \"Cues_text\" field\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Cues_text\"] = {}\n",
    "\n",
    "            # iterate over raw outputs for cues\n",
    "            for id, output in file_content[\"Outputs\"][\"Cues\"].items():\n",
    "                try:\n",
    "                    cues = extract_cues_from_output(output[0])\n",
    "                # output does not start with \"Cues:\" -> no valid cues detected\n",
    "                except SystemError:\n",
    "                    cues = []\n",
    "                # output not in bracket format -> no valid cues detected\n",
    "                except LookupError:\n",
    "                    cues = []\n",
    "\n",
    "                count_cues += len(cues)\n",
    "                file_content[\"Outputs\"][\"Cues_text\"][id] = cues\n",
    "\n",
    "        # save extracted cues to file\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)\n",
    "\n",
    "    return count_cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract cues from raw outputs\n",
    "count_cues = extract_cues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load roles model for inference\n",
    "\n",
    "You can choose the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use model from config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    roles_training_config[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Load fine tuned model\n",
    "checkpoint_dir = (\n",
    "    roles_training_config[\"output_dir\"] + \"/checkpoint-2000/\"\n",
    ")  # choose checkpoint\n",
    "model = PeftModel.from_pretrained(model, os.path.join(checkpoint_dir, \"adapter_model\"))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    roles_training_config[\"model_name_or_path\"], legacy=False\n",
    ")\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "# set langchain pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_for_roles(ds, prompt_cues, roles_prompt):\n",
    "    \"\"\"Iterates over files from output dir and prompts roles model for every extracted cue.\n",
    "\n",
    "    Args:\n",
    "        ds (datasets.Dataset): dataset for inference\n",
    "        prompt_cues (string): prompt template for cues\n",
    "        roles_prompt (string): prompt template for roles\n",
    "    \"\"\"\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    # use cue count for accurate tqdm bar\n",
    "    pbar = tqdm(total=count_cues, desc=\"Roles\")\n",
    "    # iterate over files in output dir\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        # skip zip files\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        # load file and save raw outputs for roles in \"Roles\" field\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles\"] = {}\n",
    "\n",
    "            # prompt for every extracted cue\n",
    "            for id, cues in file_content[\"Outputs\"][\"Cues_text\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles\"][id] = []\n",
    "\n",
    "                # skip if no cues were found for this sample\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                # read sentence of sample from dataset\n",
    "                sentence = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"Sentence\"]\n",
    "                # read extended sentence of sample from dataset\n",
    "                text = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended\"]\n",
    "                # preprocessing step: replace colon with period (see paper)\n",
    "                if sentence.endswith(\":\"):\n",
    "                    sentence = sentence[:-1] + \".\"\n",
    "                if text.endswith(\":\"):\n",
    "                    text = text[:-1] + \".\"\n",
    "                # build complete cue prompt with generated output\n",
    "                cue_prompt = (\n",
    "                    prompt_cues.format(Sentence=sentence)\n",
    "                    + \" Cues: \"\n",
    "                    + \", \".join([\"[\" + \", \".join(cue) + \"]\" for cue in cues])\n",
    "                    + \"</s>\"\n",
    "                )\n",
    "\n",
    "                # prompt for every cue\n",
    "                for cue in cues:\n",
    "                    file_content[\"Outputs\"][\"Roles\"][id].append([])\n",
    "                    # build roles prompt with cue prompt in context\n",
    "                    prompt = PromptTemplate(\n",
    "                        input_variables=[\"text\", \"cue\"],\n",
    "                        template=cue_prompt + \"\\nUser: \" + roles_prompt,\n",
    "                    )\n",
    "                    # prompt for roles and save in dict\n",
    "                    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "                    output = llm_chain.apply([{\"text\": text, \"cue\": \", \".join(cue)}])[\n",
    "                        0\n",
    "                    ][\"text\"]\n",
    "                    file_content[\"Outputs\"][\"Roles\"][id][-1].append(output)\n",
    "                    pbar.update()\n",
    "\n",
    "        # save raw outputs for roles in files\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)\n",
    "\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template for roles\n",
    "roles_prompt = \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: {text}\\n\\nNow find all roles in the sentence associated with the cue '{cue}' you found in the beginning sentence.\\nAssistant:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference for roles\n",
    "prompt_for_roles(test_ds, prompt_cues, roles_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free vram after inference for roles\n",
    "del tokenizer\n",
    "del model\n",
    "del pipe\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Roles and map outputs to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles_from_output(output_string: str):\n",
    "    \"\"\"Extracts detected roles from raw outputs from inference.\n",
    "\n",
    "    Args:\n",
    "        output_string (str): raw output of model\n",
    "\n",
    "    Returns:\n",
    "        res (dict): dict with all extracted roles\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize result dict\n",
    "    res = {\n",
    "        \"ptc\": \"\",\n",
    "        \"evidence\": \"\",\n",
    "        \"medium\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"addr\": \"\",\n",
    "        \"message\": \"\",\n",
    "        \"source\": \"\",\n",
    "    }\n",
    "\n",
    "    # split output on line breaks\n",
    "    output_rows = [v.strip() for v in output_string.strip().split(\"\\n\")]\n",
    "\n",
    "    # Check each line of output. Each line should start with\n",
    "    # the role name as prefix. Ignore role and leave output\n",
    "    # empty if error occurs.\n",
    "    try:\n",
    "        if output_rows[1].startswith(\"ptc: \"):\n",
    "            res[\"ptc\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[1][4:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[2].startswith(\"evidence: \"):\n",
    "            res[\"evidence\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[2][9:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[3].startswith(\"medium: \"):\n",
    "            res[\"medium\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[3][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[4].startswith(\"topic: \"):\n",
    "            res[\"topic\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[4][6:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[5].startswith(\"addr: \"):\n",
    "            res[\"addr\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[5][5:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[6].startswith(\"message: \"):\n",
    "            res[\"message\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[6][8:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[7].startswith(\"source: \"):\n",
    "            res[\"source\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[7][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # clean up result dict\n",
    "    for key, value in res.items():\n",
    "        if value == [\"\"] or value == [\"#UNK#\"]:\n",
    "            res[key] = \"\"\n",
    "        while \"#UNK#\" in value:\n",
    "            value.pop(value.index(\"#UNK#\"))\n",
    "        # remove empty strings\n",
    "        while type(value) == list and \"\" in value:\n",
    "            value.pop(value.index(\"\"))\n",
    "        res[key] = value\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles():\n",
    "    \"\"\"Iterate over all files in output dir and extract roles from raw outputs\"\"\"\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    # iterate over files\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        # skip zip files\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        # load file and save extracted roles in \"Roles_text\" field\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles_text\"] = {}\n",
    "\n",
    "            # iterate over raw outputs\n",
    "            for id, roles_for_sentence in file_content[\"Outputs\"][\"Roles\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles_text\"][id] = []\n",
    "\n",
    "                # skip if no roles were found\n",
    "                if roles_for_sentence == []:\n",
    "                    continue\n",
    "\n",
    "                for roles_output in roles_for_sentence:\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id].append([])\n",
    "\n",
    "                    # extract roles and save them\n",
    "                    roles = extract_roles_from_output(roles_output[0])\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id][-1].append(roles)\n",
    "\n",
    "        # save files with extracted roles\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract roles from raw outputs and save them to files\n",
    "extract_roles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_neighbors(i, seen, skip_index):\n",
    "    \"\"\"Counts the neighbors of word at index i that are marked for this annotation.\n",
    "\n",
    "    Args:\n",
    "        i (integer): index of current word\n",
    "        seen (list): list of booleans indicating each marked word with true\n",
    "        skip_index (integer): index of word that should not be counted in this calculation\n",
    "\n",
    "    Returns:\n",
    "        res (integer): count of neighbors that are marked for this annotation\n",
    "    \"\"\"\n",
    "    res = 0\n",
    "    if i - 2 >= 0 and i - 2 != skip_index:\n",
    "        res += 1 if seen[i - 2] else 0\n",
    "    if i - 1 >= 0 and i - 1 != skip_index:\n",
    "        res += 1 if seen[i - 1] else 0\n",
    "    if i + 1 < len(seen) and i + 1 != skip_index:\n",
    "        res += 1 if seen[i + 1] else 0\n",
    "    if i + 2 < len(seen) and i + 2 != skip_index:\n",
    "        res += 1 if seen[i + 2] else 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighborhood_swap(seen, tokens):\n",
    "    \"\"\"Tests if a word marked for this annotation has less neighbors marked for this annotation\n",
    "       than another occurrence of the same word (Levenshtein distance <= 1). Gives their indices\n",
    "       if found.\n",
    "\n",
    "    Args:\n",
    "        seen (list): list of booleans indicating each marked word with true\n",
    "        tokens (list): list of tokens of the current sample\n",
    "\n",
    "    Returns:\n",
    "        integer: first index for swap\n",
    "        integer: second index for swap\n",
    "    \"\"\"\n",
    "    for i, v in enumerate(seen):\n",
    "        # skip if word is not marked for this annotation\n",
    "        if not v:\n",
    "            continue\n",
    "\n",
    "        # count neighbors of current word that are marked for this annotation\n",
    "        neigh_c_v = count_neighbors(i, seen, -1)\n",
    "        # get indices of other words that are not marked for this annotation\n",
    "        # and have a Levenshtein distance <= 1\n",
    "        neigh = [\n",
    "            j\n",
    "            for j, t in enumerate(tokens)\n",
    "            if seen[j] == False and Levenshtein.distance(t, tokens[i]) <= 1\n",
    "        ]\n",
    "        # count neighbors for all those found words\n",
    "        neigh_c_other = [count_neighbors(n, seen, i) for n in neigh]\n",
    "\n",
    "        # if there is another word with more neighbors marked for this annotation\n",
    "        # as the current word, return its and the current words index.\n",
    "        # Continue otherwise\n",
    "        if len(neigh_c_other) > 0:\n",
    "            neigh_c_other_max = max(neigh_c_other)\n",
    "            if neigh_c_other_max > neigh_c_v:\n",
    "                return i, neigh[neigh_c_other.index(neigh_c_other_max)]\n",
    "\n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_output_list(output_list: list, ids: list, tokens: list, seen_old=None):\n",
    "    \"\"\"Maps each word of a list of words to the corresponding reference\n",
    "\n",
    "    Args:\n",
    "        output_list (list): word list for this cue or role\n",
    "        ids (list): list of sentence ids to be able to assign a token to a samples\n",
    "        tokens (list): list of sample tokens\n",
    "        seen_old (list, optional): Optional list of booleans indicating already used tokens. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        res (list): list of references for each word of output_list\n",
    "        (list): boolean list indicating already referenced tokens\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    # initialize seen list\n",
    "    seen = [False] * len(tokens)\n",
    "    if seen_old == None:\n",
    "        seen_old = [False] * len(tokens)\n",
    "\n",
    "    # iterate over words in output_list\n",
    "    for output in output_list:\n",
    "        # indices of word (output) in tokens list. Skip index if tokens is already used\n",
    "        indices = [\n",
    "            i\n",
    "            for i, v in enumerate(tokens)\n",
    "            if v == output and seen[i] == False and seen_old[i] == False\n",
    "        ]\n",
    "        # mark first occurrence of word (output) as seen if word is in tokens list\n",
    "        if len(indices) > 0:\n",
    "            seen[indices[0]] = True\n",
    "        # if word was not found in tokens list -> search again and allow all words with Levenshtein distance <= 1\n",
    "        if len(indices) == 0:\n",
    "            indices = [\n",
    "                i\n",
    "                for i, v in enumerate(tokens)\n",
    "                if seen[i] == False\n",
    "                and seen_old[i] == False\n",
    "                and Levenshtein.distance(output, v) <= 1\n",
    "            ]\n",
    "            if len(indices) > 0:\n",
    "                seen[indices[0]] = True\n",
    "\n",
    "    # keep iterating while the postprocessing (see paper) changes the output\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        # postprocessing step -> use word occurrence which is closer to other words detected for this cue/role (see paper)\n",
    "        # Iterate while changes are possible\n",
    "        i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "        while i != j:\n",
    "            seen[i] = False\n",
    "            seen[j] = True\n",
    "            changed = True\n",
    "            i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "\n",
    "        # postprocessing step -> include surrounded punctuation (see paper)\n",
    "        for i in range(len(seen)):\n",
    "            if (\n",
    "                seen[i] == False\n",
    "                and i != 0\n",
    "                and i != len(seen) - 1\n",
    "                and seen[i - 1]\n",
    "                and seen[i + 1]\n",
    "                and (\n",
    "                    tokens[i] == \",\"\n",
    "                    or tokens[i] == \":\"\n",
    "                    or tokens[i] == \";\"\n",
    "                    or tokens[i] == \"-\"\n",
    "                )\n",
    "            ):\n",
    "                seen[i] = True\n",
    "                changed = True\n",
    "\n",
    "    # build references from marked tokens\n",
    "    for i in range(len(seen)):\n",
    "        if seen[i]:\n",
    "            res.append(str(ids[i]) + \":\" + str(i))\n",
    "\n",
    "    return res, [v or seen_old[i] for i, v in enumerate(seen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs(ds):\n",
    "    \"\"\"Iterates over every file in output dir and builds and saves the annotations for it.\n",
    "\n",
    "    Args:\n",
    "        ds (datasets.Dataset): dataset which was used for inference\n",
    "    \"\"\"\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    # iterate over files in output dir\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        # skip zip files\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        # read file and save created annotations in \"Annotations\" field\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Annotations\"] = []\n",
    "\n",
    "            # iterate over predictions (extracted cues and roles)\n",
    "            for cues_text, roles_text in zip(\n",
    "                file_content[\"Outputs\"][\"Cues_text\"].items(),\n",
    "                file_content[\"Outputs\"][\"Roles_text\"].items(),\n",
    "            ):\n",
    "                id, cues = cues_text\n",
    "                id, roles_list = roles_text\n",
    "\n",
    "                # skip if there are no cues for this sample\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                # get tokens list for this sample\n",
    "                tokens = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"tokens_extended\"]\n",
    "                # get ids list for this sample to assign tokens to samples\n",
    "                ids = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended_ids\"]\n",
    "\n",
    "                seen_cues = None\n",
    "                # iterate over every predicted annotation\n",
    "                for cue, roles in zip(cues, roles_list):\n",
    "                    roles = roles[0]\n",
    "\n",
    "                    # maps cue to references list\n",
    "                    cue, seen_cues = map_output_list(cue, ids, tokens, seen_cues)\n",
    "\n",
    "                    if cue != []:\n",
    "                        # map all roles to references lists\n",
    "                        addr, _ = map_output_list(\n",
    "                            roles[\"addr\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        evidence, _ = map_output_list(\n",
    "                            roles[\"evidence\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        medium, _ = map_output_list(\n",
    "                            roles[\"medium\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        message, _ = map_output_list(\n",
    "                            roles[\"message\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        source, _ = map_output_list(\n",
    "                            roles[\"source\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        topic, _ = map_output_list(\n",
    "                            roles[\"topic\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        ptc, _ = map_output_list(\n",
    "                            roles[\"ptc\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        # build the annotation\n",
    "                        annotation = {\n",
    "                            \"Addr\": addr,\n",
    "                            \"Evidence\": evidence,\n",
    "                            \"Medium\": medium,\n",
    "                            \"Message\": message,\n",
    "                            \"Source\": source,\n",
    "                            \"Topic\": topic,\n",
    "                            \"Cue\": cue,\n",
    "                            \"PTC\": ptc,\n",
    "                        }\n",
    "                        file_content[\"Annotations\"].append(annotation)\n",
    "\n",
    "        # save the file with generated annotations\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use extracted cues and roles to build the annotations as token references and save those annotations to the files\n",
    "map_outputs(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare zip file for submission\n",
    "\n",
    "Here we create a zip file with the complete dataset in it - ready for submission in the germeval 2023 competition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./output/data/submission.zip\"):\n",
    "    os.remove(\"./output/data/submission.zip\")\n",
    "\n",
    "temp_path = \"./output/data/temp\"\n",
    "shutil.copytree(\"./output/data\", temp_path)\n",
    "\n",
    "for file in sorted(os.listdir(temp_path)):\n",
    "    file_content = {}\n",
    "\n",
    "    with open(os.path.join(temp_path, file), \"r\") as f:\n",
    "        file_content = json.load(f)\n",
    "        file_content.pop(\"Outputs\")\n",
    "\n",
    "    with open(os.path.join(temp_path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(file_content, outfile, indent=3)\n",
    "shutil.make_archive(temp_path, \"zip\", temp_path)\n",
    "shutil.move(\n",
    "    temp_path + \".zip\",\n",
    "    \"./output/data/submission.zip\",\n",
    ")\n",
    "shutil.rmtree(temp_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
